// measure-and-fix.js
// Step 1: Measures exact duration of every MP3 in public/audio/
// Step 2: Calculates perfect frame timings with zero overlap
// Step 3: Rewrites LiveDemo.tsx export with exact timings
// Run: node measure-and-fix.js

const fs = require('fs');
const path = require('path');
const { execSync } = require('child_process');

const audioDir = path.join(__dirname, 'public', 'audio');
const FPS = 30;
const GAP_FRAMES = 15; // 0.5s buffer between audio end and next scene start

// The segments we want to use, in order
const segments = [
  { id: 'live-01',  scene: 'S1' },
  { id: 'live-02',  scene: 'S2' },
  { id: 'live-03b', scene: 'S3a' },
  { id: 'live-04b', scene: 'S3b' },
  { id: 'live-05b', scene: 'S4' },
  { id: 'live-06b', scene: 'S5' },
  { id: 'live-07b', scene: 'S6' },
  { id: 'live-08b', scene: 'S7' },
  { id: 'live-09b', scene: 'S8' },
  { id: 'live-11',  scene: 'S9' },
  { id: 'live-12',  scene: 'S10' },
];

// Measure duration of an MP3 using ffprobe if available, else use file size heuristic
function getDurationSeconds(filePath) {
  try {
    const out = execSync(
      'ffprobe -v quiet -show_entries format=duration -of csv=p=0 "' + filePath + '"',
      { encoding: 'utf8', timeout: 5000 }
    ).trim();
    const d = parseFloat(out);
    if (!isNaN(d)) return d;
  } catch (e) {}

  // Fallback: MP3 bitrate heuristic (128kbps = 16000 bytes/sec)
  try {
    const size = fs.statSync(filePath).size;
    return size / 16000;
  } catch (e) {}
  return 5; // last resort default
}

// Measure all segments
console.log('Measuring MP3 durations...\n');
console.log(('Segment').padEnd(14) + ('Duration').padEnd(12) + ('Frames').padEnd(10) + 'Start Frame');
console.log('-'.repeat(48));

let cursor = 0;
const timings = [];

for (const seg of segments) {
  const filePath = path.join(audioDir, seg.id + '.mp3');
  if (!fs.existsSync(filePath)) {
    console.log(seg.id.padEnd(14) + 'MISSING - skipping');
    continue;
  }
  const durSecs = getDurationSeconds(filePath);
  const durFrames = Math.ceil(durSecs * FPS);
  const startFrame = cursor;

  console.log(
    seg.id.padEnd(14) +
    (durSecs.toFixed(2) + 's').padEnd(12) +
    (durFrames + 'f').padEnd(10) +
    startFrame
  );

  timings.push({ ...seg, durSecs, durFrames, startFrame });
  cursor += durFrames + GAP_FRAMES;
}

const totalFrames = Math.max(cursor + 30 * 30, 5400); // at least 3 min, pad closing scene
console.log('\nTotal: ' + cursor + ' frames of audio (' + (cursor/FPS/60).toFixed(1) + ' min)');
console.log('Video: ' + totalFrames + ' frames (' + (totalFrames/FPS/60).toFixed(1) + ' min)\n');

// Build scene groupings: S3a and S3b both belong to S3, so S3 spans both
// Scene starts at first segment start, ends at last segment end + gap
const sceneMap = {
  S1:  { comp: 'S1',  segs: ['live-01'] },
  S2:  { comp: 'S2',  segs: ['live-02'] },
  S3:  { comp: 'S3',  segs: ['live-03b', 'live-04b'] },
  S4:  { comp: 'S4',  segs: ['live-05b'] },
  S5:  { comp: 'S5',  segs: ['live-06b'] },
  S6:  { comp: 'S6',  segs: ['live-07b'] },
  S7:  { comp: 'S7',  segs: ['live-08b'] },
  S8:  { comp: 'S8',  segs: ['live-09b'] },
  S9:  { comp: 'S9',  segs: ['live-11'] },
  S10: { comp: 'S10', segs: ['live-12'] },
};

// Calculate scene from/duration based on measured timings
const scenes = [];
for (const [sceneName, scene] of Object.entries(sceneMap)) {
  const firstSeg = timings.find(t => t.id === scene.segs[0]);
  const lastSeg  = timings.find(t => t.id === scene.segs[scene.segs.length - 1]);
  if (!firstSeg) continue;

  const from = firstSeg.startFrame;
  const nextSceneName = Object.keys(sceneMap)[Object.keys(sceneMap).indexOf(sceneName) + 1];
  const nextSceneSegs = nextSceneName ? sceneMap[nextSceneName].segs : null;
  const nextFirst = nextSceneSegs ? timings.find(t => t.id === nextSceneSegs[0]) : null;
  const duration = nextFirst
    ? nextFirst.startFrame - from
    : totalFrames - from;

  scenes.push({ name: sceneName, comp: scene.comp, from, duration });
}

// Generate the export section
const audioLines = timings.map(t =>
  '    <Sequence from={' + t.startFrame + '}>' +
  '<Audio src={staticFile(\'audio/' + t.id + '.mp3\')} volume={1} /></Sequence>'
).join('\n');

const sceneLines = scenes.map(s =>
  '    <Sequence from={' + s.from + '} durationInFrames={' + s.duration + '}><' + s.comp + ' /></Sequence>'
).join('\n');

const newExport = `// \u2500\u2500 Main export \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500
// AUTO-GENERATED by measure-and-fix.js
// Audio timings measured from actual MP3 file durations - zero overlap guaranteed
// Total: ${totalFrames} frames = ${(totalFrames/FPS/60).toFixed(1)} minutes @ ${FPS}fps
export const LiveDemo: React.FC = () => (
  <AbsoluteFill>
    {/* Audio - each segment starts after previous one ends */}
${audioLines}

    {/* Scenes - each scene spans its audio segment(s) exactly */}
${sceneLines}
  </AbsoluteFill>
);
`;

// Patch LiveDemo.tsx
const livePath = path.join(__dirname, 'src', 'LiveDemo', 'LiveDemo.tsx');
let live = fs.readFileSync(livePath, 'utf8');

// Find the last occurrence of the export marker or export const LiveDemo
let cutIdx = live.lastIndexOf('// \u2500\u2500 Main export');
if (cutIdx === -1) cutIdx = live.lastIndexOf('export const LiveDemo');
if (cutIdx === -1) {
  console.error('Cannot find export section in LiveDemo.tsx');
  process.exit(1);
}

fs.writeFileSync(livePath, live.slice(0, cutIdx) + newExport);
console.log('LiveDemo.tsx rewritten with exact timings.');

// Also fix Root.tsx duration
const rootPath = path.join(__dirname, 'src', 'Root.tsx');
let rootContent = fs.readFileSync(rootPath, 'utf8');
rootContent = rootContent.replace(
  /(<Composition[\s\S]*?id="LiveDemo"[\s\S]*?durationInFrames=\{)\d+(\})/,
  '$1' + totalFrames + '$2'
);
fs.writeFileSync(rootPath, rootContent);
console.log('Root.tsx updated: LiveDemo duration = ' + totalFrames + ' frames');
console.log('\nAll done. Run: npx remotion studio --force-new');
